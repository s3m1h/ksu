# -*- coding: utf-8 -*-
"""Top2Vec_ödevl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vKz3Oqwl16gcpZwkmdoIG69GSNU3BGRQ

# Top2Vec ödev

## top2vec paketlerinin kurulması
"""

!pip install top2vec top2vec[sentence_encoders] top2vec[sentence_transformers] top2vec[indexing]

"""## Paketlerin import edilmesi"""

import numpy as np
import pandas as pd
from top2vec import Top2Vec
np.random.seed(0)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('turkish')

"""## Verilerin okunması

* dosyalarımızın içerisinde ilk 2 satırdaki makale adı ve anahtar kelime ile başlayan satırları atlıyoruz
"""

import glob
files = glob.glob("makaleler/*.txt")

original_texts = []
for file in files:
  with open(file, "r") as f:
    rows = f.readlines()[2:]
    text = ''
    for row in rows:
      text += row
    original_texts.append(text)
print("makale sayisi: ",len(original_texts))
original_texts[0]

"""## internetten indirdiğimiz makaleleri okuma"""

files = glob.glob("makaleler_diger/*.txt")
original_texts_diger = []
for file in files:
    with open(file, "r") as dosya:
        original_texts_diger.append(dosya.read())
print("makale sayisi: ",len(original_texts_diger))
for text in original_texts_diger:
  print(text,"\n")

"""## Ön işleme adımı - verimizi ön işleme adımlarından geçiriyoruz.

"""

import re
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words("turkish")
def norm_doc(single_doc):
  single_doc = re.sub(" \d+", " ",single_doc)
  single_doc = re.sub(r"[{}]".format(",.;"), "" ,single_doc)
  single_doc = re.sub(r"\[", "" ,single_doc)
  single_doc = re.sub(r"\]", "" ,single_doc)
  single_doc = single_doc.lower()
  single_doc = single_doc.strip()
  #punctuations = [')','(',',',':','),',').','.','-',';','.,']
  single_doc = single_doc.replace("✓", "")


  tokens = WPT.tokenize(single_doc)
  filtered_tokens = [token for token in tokens if token not in stop_word_list]
  single_doc = ' '.join(filtered_tokens)
  return single_doc

"""* Yukarıda ön işleme adımları için yazdığımız fonksiyonu daha önce okumuş olduğumuz makalelere uygulayıp yeni bir listeye yüklüyoruz"""

import pprint
temizlenmis_metinler = []
for text in original_texts:
    temizlenmis_metinler.append(norm_doc(text))
for metin in temizlenmis_metinler:
    pprint.pprint(metin)
    print("\n")

temizlenmis_metinler

"""## internetten indirdiğimiz makaleleri ön işlem adımlarından geçirelim"""

temizlenmis_makaleler = []
for makale in makaleler:
    temizlenmis_makaleler.append(norm_doc(makale))

len(temizlenmis_makaleler)

from top2vec import Top2Vec
# embedding_model='universal-sentence-encoder'
model = Top2Vec(temizlenmis_makaleler,min_count=10,speed="learn",embedding_model='universal-sentence-encoder')

topic_sizes, topic_nums = model.get_topic_sizes()
print(topic_sizes)

"""## Konu Sayısını Görüntüleme"""

model.get_num_topics()

"""## Her Konu İçin Anahtar Kelime Alma

---


"""

model.topic_words

len(model.topic_words[1])

"""## Creating Topic Wordclouds"""

model.generate_topic_wordcloud(0)

"""## Accessing Topic Vectors"""

model.topic_vectors

"""## Searching for Topics by Keyword"""

topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=["zaman"], num_topics=2)

topic_words, topic_scores

for topic in topic_nums:
    model.generate_topic_wordcloud(topic)

documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=["dil", "yapay","egitmek","dogal"], num_docs=5)
for doc, score, doc_id in zip(documents, document_scores, document_ids):
    print(f"Document: {doc_id}, Score: {score}")
    print("-----------")
    print(doc)
    print("-----------")
    print()

